{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Performance Analysis\n",
    "\n",
    "Visualize PKI performance test results from `data/perf-metrics/`.\n",
    "\n",
    "## How Performance Tests Work\n",
    "\n",
    "The `scripts/perf-test.py` orchestrator uses an **in-container batch execution strategy** to minimize overhead:\n",
    "\n",
    "1. Generates a shell script with all certificate operations (issue + revoke + CRL)\n",
    "2. Copies it into the CA container via `sudo podman cp`\n",
    "3. Runs it with a single `sudo podman exec` call\n",
    "4. Parses structured timing data from stdout\n",
    "\n",
    "### Metrics Captured\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `issued` | Total certificates issued per PKI type |\n",
    "| `revoked` | Total certificates revoked per PKI type |\n",
    "| `issue_rate` | Issuance throughput (certs/sec) |\n",
    "| `revoke_rate` | Revocation throughput (certs/sec) |\n",
    "| `issue_p50/p90/p95/p99` | Issuance latency percentiles (ms) |\n",
    "| `total_duration` | Total test duration (seconds) |\n",
    "\n",
    "### Default Certificate Distribution\n",
    "\n",
    "| PKI Type | Share |\n",
    "|----------|-------|\n",
    "| RSA-4096 | 40% |\n",
    "| ECC P-384 | 30% |\n",
    "| ML-DSA-87 | 30% |\n",
    "\n",
    "### Running a Test\n",
    "\n",
    "```bash\n",
    "# Quick test (100 certs, RSA only)\n",
    "./lab perf-test --count 100 --pki-types rsa\n",
    "\n",
    "# Full test (10K certs across all PKI types, 10% revocation)\n",
    "./lab perf-test --count 10000 --revoke-pct 10 --pki-types rsa,ecc,pqc\n",
    "```\n",
    "\n",
    "Results are written to `data/perf-metrics/latest.json` (and a timestamped copy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-docs",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Performance metrics are mounted read-only at `/home/jovyan/perf-metrics/` inside the Jupyter container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "METRICS_DIR = Path(\"/home/jovyan/perf-metrics\")\n",
    "\n",
    "print(f\"Metrics directory: {METRICS_DIR}\")\n",
    "if METRICS_DIR.exists():\n",
    "    files = sorted(METRICS_DIR.glob(\"*.json\"))\n",
    "    print(f\"Found {len(files)} result file(s): {[f.name for f in files]}\")\n",
    "else:\n",
    "    print(\"Metrics directory not found. Run ./lab perf-test to generate data.\")\n",
    "    files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Latest Results\n",
    "\n",
    "Parse `latest.json` and display a summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_file = METRICS_DIR / \"latest.json\"\n",
    "data = None\n",
    "\n",
    "if latest_file.exists():\n",
    "    with open(latest_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Display top-level summary\n",
    "    if isinstance(data, dict):\n",
    "        # Show metadata\n",
    "        for key in [\"timestamp\", \"total_duration\", \"total_issued\", \"total_revoked\"]:\n",
    "            if key in data:\n",
    "                print(f\"{key}: {data[key]}\")\n",
    "\n",
    "        # Per-PKI summary\n",
    "        pki_results = data.get(\"results\", data.get(\"pki_types\", {}))\n",
    "        if isinstance(pki_results, dict):\n",
    "            rows = []\n",
    "            for pki, metrics in pki_results.items():\n",
    "                if isinstance(metrics, dict):\n",
    "                    rows.append({\"pki_type\": pki, **metrics})\n",
    "            if rows:\n",
    "                print(\"\\nPer-PKI Summary:\")\n",
    "                display(pd.DataFrame(rows).set_index(\"pki_type\"))\n",
    "        elif isinstance(pki_results, list):\n",
    "            print(\"\\nPer-PKI Summary:\")\n",
    "            display(pd.DataFrame(pki_results))\n",
    "        else:\n",
    "            print(json.dumps(data, indent=2))\n",
    "    else:\n",
    "        print(json.dumps(data, indent=2))\n",
    "else:\n",
    "    print(\"No latest.json found. Run ./lab perf-test to generate data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "throughput-header",
   "metadata": {},
   "source": [
    "## Throughput Comparison\n",
    "\n",
    "Issuance and revocation rates (certs/sec) per PKI type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "throughput-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data and isinstance(data, dict):\n",
    "    pki_results = data.get(\"results\", data.get(\"pki_types\", {}))\n",
    "    if isinstance(pki_results, dict):\n",
    "        rows = []\n",
    "        for pki, metrics in pki_results.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                rows.append({\n",
    "                    \"PKI Type\": pki.upper(),\n",
    "                    \"Issue Rate (certs/s)\": metrics.get(\"issue_rate\", 0),\n",
    "                    \"Revoke Rate (certs/s)\": metrics.get(\"revoke_rate\", 0),\n",
    "                })\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows).set_index(\"PKI Type\")\n",
    "            print(\"Throughput Comparison:\")\n",
    "            display(df)\n",
    "\n",
    "            # Simple text bar chart\n",
    "            print(\"\\nIssuance Rate:\")\n",
    "            max_rate = max(r[\"Issue Rate (certs/s)\"] for r in rows) or 1\n",
    "            for r in rows:\n",
    "                bar_len = int(40 * r[\"Issue Rate (certs/s)\"] / max_rate)\n",
    "                bar = \"\u2588\" * bar_len\n",
    "                print(f\"  {r['PKI Type']:6s} {bar} {r['Issue Rate (certs/s)']:.1f}/s\")\n",
    "\n",
    "            print(\"\\nRevocation Rate:\")\n",
    "            max_rate = max(r[\"Revoke Rate (certs/s)\"] for r in rows) or 1\n",
    "            for r in rows:\n",
    "                bar_len = int(40 * r[\"Revoke Rate (certs/s)\"] / max_rate)\n",
    "                bar = \"\u2588\" * bar_len\n",
    "                print(f\"  {r['PKI Type']:6s} {bar} {r['Revoke Rate (certs/s)']:.1f}/s\")\n",
    "        else:\n",
    "            print(\"No per-PKI throughput data found.\")\n",
    "    else:\n",
    "        print(\"Unexpected results format.\")\n",
    "else:\n",
    "    print(\"No data loaded. Run the cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latency-header",
   "metadata": {},
   "source": [
    "## Latency Percentiles\n",
    "\n",
    "Issuance latency distribution (p50 / p90 / p95 / p99) per PKI type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latency-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data and isinstance(data, dict):\n",
    "    pki_results = data.get(\"results\", data.get(\"pki_types\", {}))\n",
    "    if isinstance(pki_results, dict):\n",
    "        rows = []\n",
    "        percentile_keys = [\"issue_p50\", \"issue_p90\", \"issue_p95\", \"issue_p99\"]\n",
    "        for pki, metrics in pki_results.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                row = {\"PKI Type\": pki.upper()}\n",
    "                for pk in percentile_keys:\n",
    "                    label = pk.replace(\"issue_\", \"\").upper()\n",
    "                    row[label] = f\"{metrics.get(pk, 0):.1f} ms\"\n",
    "                rows.append(row)\n",
    "\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows).set_index(\"PKI Type\")\n",
    "            print(\"Issuance Latency Percentiles:\")\n",
    "            display(df)\n",
    "        else:\n",
    "            print(\"No latency percentile data found.\")\n",
    "    else:\n",
    "        print(\"Unexpected results format.\")\n",
    "else:\n",
    "    print(\"No data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase-header",
   "metadata": {},
   "source": [
    "## Phase Breakdown\n",
    "\n",
    "Duration breakdown by phase (issuance, revocation, CRL generation) per PKI type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data and isinstance(data, dict):\n",
    "    pki_results = data.get(\"results\", data.get(\"pki_types\", {}))\n",
    "    if isinstance(pki_results, dict):\n",
    "        rows = []\n",
    "        for pki, metrics in pki_results.items():\n",
    "            if isinstance(metrics, dict):\n",
    "                row = {\"PKI Type\": pki.upper()}\n",
    "                for phase in [\"issue_duration\", \"revoke_duration\", \"crl_duration\", \"total_duration\"]:\n",
    "                    label = phase.replace(\"_duration\", \"\").replace(\"_\", \" \").title()\n",
    "                    val = metrics.get(phase, metrics.get(phase.replace(\"_duration\", \"_time\"), 0))\n",
    "                    row[label] = f\"{val:.2f}s\" if isinstance(val, (int, float)) else str(val)\n",
    "                rows.append(row)\n",
    "\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows).set_index(\"PKI Type\")\n",
    "            print(\"Phase Breakdown:\")\n",
    "            display(df)\n",
    "        else:\n",
    "            print(\"No phase duration data found.\")\n",
    "    else:\n",
    "        print(\"Unexpected results format.\")\n",
    "else:\n",
    "    print(\"No data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-header",
   "metadata": {},
   "source": [
    "## Historical Trend\n",
    "\n",
    "If multiple timestamped result files exist, plot throughput over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-trend",
   "metadata": {},
   "outputs": [],
   "source": [
    "if METRICS_DIR.exists():\n",
    "    result_files = sorted(\n",
    "        [f for f in METRICS_DIR.glob(\"*.json\") if f.name != \"latest.json\"],\n",
    "        key=lambda f: f.stat().st_mtime,\n",
    "    )\n",
    "\n",
    "    if len(result_files) >= 2:\n",
    "        trend_rows = []\n",
    "        for rf in result_files:\n",
    "            try:\n",
    "                with open(rf) as f:\n",
    "                    rd = json.load(f)\n",
    "                ts = rd.get(\"timestamp\", rf.stem)\n",
    "                pki_results = rd.get(\"results\", rd.get(\"pki_types\", {}))\n",
    "                if isinstance(pki_results, dict):\n",
    "                    for pki, metrics in pki_results.items():\n",
    "                        if isinstance(metrics, dict):\n",
    "                            trend_rows.append({\n",
    "                                \"timestamp\": ts,\n",
    "                                \"file\": rf.name,\n",
    "                                \"pki_type\": pki.upper(),\n",
    "                                \"issue_rate\": metrics.get(\"issue_rate\", 0),\n",
    "                                \"revoke_rate\": metrics.get(\"revoke_rate\", 0),\n",
    "                            })\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                continue\n",
    "\n",
    "        if trend_rows:\n",
    "            df = pd.DataFrame(trend_rows)\n",
    "            print(f\"Historical results from {len(result_files)} test runs:\")\n",
    "            display(df.set_index([\"timestamp\", \"pki_type\"]))\n",
    "        else:\n",
    "            print(\"Could not parse historical data.\")\n",
    "    else:\n",
    "        print(f\"Only {len(result_files)} timestamped result file(s) found.\")\n",
    "        print(\"Run ./lab perf-test multiple times to build historical data.\")\n",
    "else:\n",
    "    print(\"Metrics directory not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## Run Performance Test\n",
    "\n",
    "Performance tests must be run from the lab host terminal (they use `sudo podman exec`). After running a test, re-execute the cells above to see updated results.\n",
    "\n",
    "```bash\n",
    "# Quick test (100 certs, RSA)\n",
    "./lab perf-test --count 100 --pki-types rsa\n",
    "\n",
    "# Multi-PKI test (1000 certs, 10% revocation)\n",
    "./lab perf-test --count 1000 --revoke-pct 10 --pki-types rsa,ecc,pqc\n",
    "\n",
    "# Large-scale test\n",
    "./lab perf-test --count 10000 --revoke-pct 10 --pki-types rsa,ecc,pqc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refresh-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running a perf test, re-run this cell to reload\n",
    "latest_file = METRICS_DIR / \"latest.json\"\n",
    "if latest_file.exists():\n",
    "    mtime = datetime.fromtimestamp(latest_file.stat().st_mtime)\n",
    "    print(f\"latest.json last modified: {mtime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    with open(latest_file) as f:\n",
    "        data = json.load(f)\n",
    "    print(\"Data reloaded. Re-run the analysis cells above.\")\n",
    "else:\n",
    "    print(\"No results file found yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}