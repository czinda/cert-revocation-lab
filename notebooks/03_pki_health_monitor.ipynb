{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# PKI Health Monitor\n",
    "\n",
    "This notebook queries the **PKI Exporter** Prometheus-format metrics endpoint and displays CA health status across all three PKI hierarchies (RSA-4096, ECC P-384, ML-DSA-87).\n",
    "\n",
    "## Monitoring Stack Architecture\n",
    "\n",
    "```\n",
    "Dogtag CAs (9 targets) \u2192 PKI Exporter (:9091/metrics) \u2192 Prometheus (:9090) \u2192 Grafana (:3000)\n",
    "```\n",
    "\n",
    "The PKI Exporter scrapes all Dogtag CAs every 15 seconds and exposes Prometheus-format metrics. This notebook reads those metrics directly from the exporter.\n",
    "\n",
    "### CAs Scraped\n",
    "\n",
    "| PKI Type | Root CA | Intermediate CA | IoT CA | EST CA |\n",
    "|----------|---------|-----------------|--------|--------|\n",
    "| **RSA-4096** | 8443 | 8444 | 8445 | 8447 |\n",
    "| **ECC P-384** | 8463 | 8464 | 8465 | 8466 |\n",
    "| **ML-DSA-87** | 8453 | 8454 | 8455 | 8456 |\n",
    "\n",
    "### Metrics Available\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `pki_ca_up` | CA reachability (1=up, 0=down) |\n",
    "| `pki_certificates_total` | Certificate count by status (VALID/REVOKED) |\n",
    "| `pki_crl_last_update_timestamp` | Last CRL generation time |\n",
    "| `pki_crl_next_update_timestamp` | Next CRL generation time |\n",
    "| `pki_crl_entries_total` | Number of entries in CRL |\n",
    "| `pki_ocsp_response_seconds` | OCSP response latency |\n",
    "| `pki_issuance_rate` | Certificate issuance rate (from perf-test) |\n",
    "| `pki_revocation_rate` | Certificate revocation rate (from perf-test) |\n",
    "| `pki_issuance_duration_seconds` | Issuance latency percentiles |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-docs",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The Jupyter container is on the same Docker network as the PKI Exporter, so we can reach it by hostname.\n",
    "\n",
    "| Variable | Default | Description |\n",
    "|----------|---------|-------------|\n",
    "| `PKI_EXPORTER_URL` | `http://pki-exporter.cert-lab.local:9091` | PKI Exporter base URL |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "PKI_EXPORTER_URL = \"http://pki-exporter.cert-lab.local:9091\"\n",
    "\n",
    "print(f\"PKI Exporter URL: {PKI_EXPORTER_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metrics():\n",
    "    \"\"\"Fetch and parse Prometheus text format metrics into a list of dicts.\"\"\"\n",
    "    resp = httpx.get(f\"{PKI_EXPORTER_URL}/metrics\", timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    return parse_prometheus(resp.text)\n",
    "\n",
    "\n",
    "def parse_prometheus(text):\n",
    "    \"\"\"Parse Prometheus exposition format into structured records.\"\"\"\n",
    "    records = []\n",
    "    # Matches: metric_name{label=\"val\",...} value\n",
    "    pattern = re.compile(\n",
    "        r'^([a-zA-Z_:][a-zA-Z0-9_:]*)'\n",
    "        r'(?:\\{([^}]*)\\})?'\n",
    "        r'\\s+([\\d.eE+-]+(?:NaN)?)$'\n",
    "    )\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#'):\n",
    "            continue\n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            name = m.group(1)\n",
    "            labels_str = m.group(2) or \"\"\n",
    "            value = m.group(3)\n",
    "            labels = {}\n",
    "            if labels_str:\n",
    "                for pair in re.findall(r'(\\w+)=\"([^\"]*)\"', labels_str):\n",
    "                    labels[pair[0]] = pair[1]\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            records.append({\"metric\": name, \"value\": value, **labels})\n",
    "    return records\n",
    "\n",
    "\n",
    "def get_metric(records, name, **label_filters):\n",
    "    \"\"\"Filter parsed metrics by name and optional label values.\"\"\"\n",
    "    results = [r for r in records if r[\"metric\"] == name]\n",
    "    for k, v in label_filters.items():\n",
    "        results = [r for r in results if r.get(k) == v]\n",
    "    return results\n",
    "\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    metrics = fetch_metrics()\n",
    "    print(f\"Connected to PKI Exporter. Parsed {len(metrics)} metric samples.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to PKI Exporter: {e}\")\n",
    "    print(\"Make sure the monitoring stack is running (Phase 10 of start-lab.sh).\")\n",
    "    metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "health-header",
   "metadata": {},
   "source": [
    "## CA Health Status\n",
    "\n",
    "Shows whether each CA is reachable. A value of `1` means the CA responded to a health check; `0` means it is down or unreachable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "health-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics:\n",
    "    health = get_metric(metrics, \"pki_ca_up\")\n",
    "    if health:\n",
    "        df = pd.DataFrame(health)\n",
    "        df[\"status\"] = df[\"value\"].apply(lambda v: \"UP\" if v == 1 else \"DOWN\")\n",
    "        cols = [c for c in [\"pki_type\", \"ca_level\", \"status\"] if c in df.columns]\n",
    "        if cols:\n",
    "            pivot = df.pivot_table(\n",
    "                index=\"ca_level\", columns=\"pki_type\",\n",
    "                values=\"status\", aggfunc=\"first\"\n",
    "            )\n",
    "            # Reorder columns and rows for readability\n",
    "            for col_order in [[\"rsa\", \"ecc\", \"pq\"], [\"rsa\", \"ecc\"], [\"rsa\"]]:\n",
    "                available = [c for c in col_order if c in pivot.columns]\n",
    "                if available:\n",
    "                    pivot = pivot[available]\n",
    "                    break\n",
    "            row_order = [\"root\", \"intermediate\", \"iot\", \"est\", \"acme\"]\n",
    "            available_rows = [r for r in row_order if r in pivot.index]\n",
    "            pivot = pivot.loc[available_rows]\n",
    "            print(\"CA Health Status (pki_type x ca_level):\")\n",
    "            display(pivot)\n",
    "        else:\n",
    "            display(df)\n",
    "    else:\n",
    "        print(\"No pki_ca_up metrics found.\")\n",
    "else:\n",
    "    print(\"No metrics loaded. Run the cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inventory-header",
   "metadata": {},
   "source": [
    "## Certificate Inventory\n",
    "\n",
    "Number of **VALID** and **REVOKED** certificates per CA, as reported by the Dogtag REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inventory-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics:\n",
    "    certs = get_metric(metrics, \"pki_certificates_total\")\n",
    "    if certs:\n",
    "        df = pd.DataFrame(certs)\n",
    "        df[\"count\"] = df[\"value\"].astype(int)\n",
    "        cols = [c for c in [\"pki_type\", \"ca_level\", \"status\", \"count\"] if c in df.columns]\n",
    "        if \"status\" in df.columns:\n",
    "            pivot = df.pivot_table(\n",
    "                index=[\"pki_type\", \"ca_level\"], columns=\"status\",\n",
    "                values=\"count\", aggfunc=\"sum\", fill_value=0\n",
    "            )\n",
    "            print(\"Certificate Inventory:\")\n",
    "            display(pivot)\n",
    "        else:\n",
    "            display(df[cols])\n",
    "    else:\n",
    "        print(\"No pki_certificates_total metrics found.\")\n",
    "else:\n",
    "    print(\"No metrics loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crl-header",
   "metadata": {},
   "source": [
    "## CRL Status\n",
    "\n",
    "Certificate Revocation List timing and entry count for each CA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crl-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics:\n",
    "    crl_last = get_metric(metrics, \"pki_crl_last_update_timestamp\")\n",
    "    crl_next = get_metric(metrics, \"pki_crl_next_update_timestamp\")\n",
    "    crl_entries = get_metric(metrics, \"pki_crl_entries_total\")\n",
    "\n",
    "    if crl_last or crl_next or crl_entries:\n",
    "        rows = []\n",
    "        # Index by (pki_type, ca_level)\n",
    "        lookup_next = {(r.get(\"pki_type\"), r.get(\"ca_level\")): r[\"value\"] for r in crl_next}\n",
    "        lookup_entries = {(r.get(\"pki_type\"), r.get(\"ca_level\")): r[\"value\"] for r in crl_entries}\n",
    "\n",
    "        for r in crl_last:\n",
    "            key = (r.get(\"pki_type\"), r.get(\"ca_level\"))\n",
    "            last_ts = r[\"value\"]\n",
    "            next_ts = lookup_next.get(key, None)\n",
    "            entries = lookup_entries.get(key, None)\n",
    "            rows.append({\n",
    "                \"pki_type\": key[0],\n",
    "                \"ca_level\": key[1],\n",
    "                \"last_update\": datetime.fromtimestamp(last_ts).strftime(\"%Y-%m-%d %H:%M:%S\") if last_ts and last_ts > 0 else \"N/A\",\n",
    "                \"next_update\": datetime.fromtimestamp(next_ts).strftime(\"%Y-%m-%d %H:%M:%S\") if next_ts and next_ts > 0 else \"N/A\",\n",
    "                \"entries\": int(entries) if entries is not None else \"N/A\",\n",
    "            })\n",
    "\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            print(\"CRL Status:\")\n",
    "            display(df.set_index([\"pki_type\", \"ca_level\"]))\n",
    "        else:\n",
    "            print(\"No CRL data parsed.\")\n",
    "    else:\n",
    "        print(\"No CRL metrics found.\")\n",
    "else:\n",
    "    print(\"No metrics loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ocsp-header",
   "metadata": {},
   "source": [
    "## OCSP Latency\n",
    "\n",
    "OCSP response time per CA. Thresholds: **green** < 200ms, **yellow** < 500ms, **red** \u2265 500ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ocsp-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics:\n",
    "    ocsp = get_metric(metrics, \"pki_ocsp_response_seconds\")\n",
    "    if ocsp:\n",
    "        rows = []\n",
    "        for r in ocsp:\n",
    "            latency_ms = r[\"value\"] * 1000\n",
    "            if latency_ms < 200:\n",
    "                indicator = \"[OK]\"\n",
    "            elif latency_ms < 500:\n",
    "                indicator = \"[WARN]\"\n",
    "            else:\n",
    "                indicator = \"[SLOW]\"\n",
    "            rows.append({\n",
    "                \"pki_type\": r.get(\"pki_type\", \"?\"),\n",
    "                \"ca_level\": r.get(\"ca_level\", \"?\"),\n",
    "                \"latency_ms\": f\"{latency_ms:.1f}\",\n",
    "                \"status\": indicator,\n",
    "            })\n",
    "        df = pd.DataFrame(rows)\n",
    "        print(\"OCSP Response Latency:\")\n",
    "        display(df.set_index([\"pki_type\", \"ca_level\"]))\n",
    "    else:\n",
    "        print(\"No pki_ocsp_response_seconds metrics found.\")\n",
    "else:\n",
    "    print(\"No metrics loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perf-header",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Issuance and revocation rates and latency percentiles from the last performance test run. These metrics are generated by `./lab perf-test` and exposed via the PKI Exporter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perf-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics:\n",
    "    # Throughput rates\n",
    "    issue_rate = get_metric(metrics, \"pki_issuance_rate\")\n",
    "    revoke_rate = get_metric(metrics, \"pki_revocation_rate\")\n",
    "    issue_total = get_metric(metrics, \"pki_issuance_total\")\n",
    "    revoke_total = get_metric(metrics, \"pki_revocation_total\")\n",
    "\n",
    "    if issue_rate or revoke_rate:\n",
    "        rows = []\n",
    "        rate_lookup = {r.get(\"pki_type\"): r[\"value\"] for r in revoke_rate}\n",
    "        total_issue = {r.get(\"pki_type\"): int(r[\"value\"]) for r in issue_total}\n",
    "        total_revoke = {r.get(\"pki_type\"): int(r[\"value\"]) for r in revoke_total}\n",
    "        for r in issue_rate:\n",
    "            pki = r.get(\"pki_type\", \"?\")\n",
    "            rows.append({\n",
    "                \"pki_type\": pki,\n",
    "                \"issued\": total_issue.get(pki, \"N/A\"),\n",
    "                \"revoked\": total_revoke.get(pki, \"N/A\"),\n",
    "                \"issue_rate_per_sec\": f\"{r['value']:.2f}\",\n",
    "                \"revoke_rate_per_sec\": f\"{rate_lookup.get(pki, 0):.2f}\",\n",
    "            })\n",
    "        if rows:\n",
    "            print(\"Performance Throughput:\")\n",
    "            display(pd.DataFrame(rows).set_index(\"pki_type\"))\n",
    "    else:\n",
    "        print(\"No throughput metrics found. Run ./lab perf-test to generate data.\")\n",
    "\n",
    "    # Latency percentiles\n",
    "    latency = get_metric(metrics, \"pki_issuance_duration_seconds\")\n",
    "    if latency:\n",
    "        rows = []\n",
    "        for r in latency:\n",
    "            rows.append({\n",
    "                \"pki_type\": r.get(\"pki_type\", \"?\"),\n",
    "                \"quantile\": r.get(\"quantile\", \"?\"),\n",
    "                \"latency_ms\": f\"{r['value'] * 1000:.1f}\",\n",
    "            })\n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            pivot = df.pivot_table(\n",
    "                index=\"pki_type\", columns=\"quantile\",\n",
    "                values=\"latency_ms\", aggfunc=\"first\"\n",
    "            )\n",
    "            print(\"\\nIssuance Latency Percentiles (ms):\")\n",
    "            display(pivot)\n",
    "    else:\n",
    "        print(\"No latency percentile metrics found.\")\n",
    "else:\n",
    "    print(\"No metrics loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refresh-header",
   "metadata": {},
   "source": [
    "## Auto-Refresh Dashboard\n",
    "\n",
    "Polls the PKI Exporter every 15 seconds and displays a summary. **Interrupt the kernel** (stop button) to halt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auto-refresh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dashboard_summary(metrics):\n",
    "    \"\"\"Print a compact health summary.\"\"\"\n",
    "    # CA health\n",
    "    health = get_metric(metrics, \"pki_ca_up\")\n",
    "    up = sum(1 for h in health if h[\"value\"] == 1)\n",
    "    total = len(health)\n",
    "    print(f\"CAs: {up}/{total} UP\")\n",
    "\n",
    "    # Certificate counts\n",
    "    certs = get_metric(metrics, \"pki_certificates_total\")\n",
    "    valid = sum(int(c[\"value\"]) for c in certs if c.get(\"status\") == \"VALID\")\n",
    "    revoked = sum(int(c[\"value\"]) for c in certs if c.get(\"status\") == \"REVOKED\")\n",
    "    print(f\"Certificates: {valid} valid, {revoked} revoked\")\n",
    "\n",
    "    # OCSP average\n",
    "    ocsp = get_metric(metrics, \"pki_ocsp_response_seconds\")\n",
    "    if ocsp:\n",
    "        avg_ms = sum(o[\"value\"] for o in ocsp) / len(ocsp) * 1000\n",
    "        print(f\"OCSP avg latency: {avg_ms:.1f} ms\")\n",
    "\n",
    "    # Per-PKI breakdown\n",
    "    for pki in [\"rsa\", \"ecc\", \"pq\"]:\n",
    "        pki_health = [h for h in health if h.get(\"pki_type\") == pki]\n",
    "        if pki_health:\n",
    "            pki_up = sum(1 for h in pki_health if h[\"value\"] == 1)\n",
    "            print(f\"  {pki.upper()}: {pki_up}/{len(pki_health)} CAs up\")\n",
    "\n",
    "\n",
    "POLL_INTERVAL = 15  # seconds\n",
    "print(f\"Auto-refreshing every {POLL_INTERVAL}s. Interrupt kernel to stop.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"PKI Health Dashboard  [{ts}]  (refreshes every {POLL_INTERVAL}s)\")\n",
    "        print(\"=\" * 50)\n",
    "        try:\n",
    "            m = fetch_metrics()\n",
    "            dashboard_summary(m)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching metrics: {e}\")\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nDashboard stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}